{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1,  7.4,  0. ],\n",
       "       [-0.2,  5.3,  0. ],\n",
       "       [ 0.2,  8.2,  1. ],\n",
       "       [ 0.2,  7.7,  1. ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([\n",
    "    [0.1, 7.4, 0],\n",
    "    [-0.2, 5.3, 0],\n",
    "    [0.2, 8.2, 1],\n",
    "    [0.2, 7.7, 1]])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1000,  7.4000,  0.0000],\n",
       "        [-0.2000,  5.3000,  0.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载器从数据集中选择了 2 个样本。 \n",
    "\n",
    "这些样本被转换为张量（2 个大小为 3 的样本）。\n",
    "\n",
    "创建并返回一个新的张量 (2x3)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x1': 0.1, 'x2': 7.4, 'y': 0},\n",
      " {'x1': -0.2, 'x2': 5.3, 'y': 0},\n",
      " {'x1': 0.2, 'x2': 8.2, 'y': 1},\n",
      " {'x1': 0.2, 'x2': 7.7, 'y': 10}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'x1': 0.1, 'x2': 7.4, 'y': 0},\n",
       " {'x1': -0.2, 'x2': 5.3, 'y': 0},\n",
       " {'x1': 0.2, 'x2': 8.2, 'y': 1},\n",
       " {'x1': 0.2, 'x2': 7.7, 'y': 10}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# now dataset is a list of dicts\n",
    "dict_data = [\n",
    "    {'x1': 0.1, 'x2': 7.4, 'y': 0},\n",
    "    {'x1': -0.2, 'x2': 5.3, 'y': 0},\n",
    "    {'x1': 0.2, 'x2': 8.2, 'y': 1},\n",
    "    {'x1': 0.2, 'x2': 7.7, 'y': 10},\n",
    "]\n",
    "pprint(dict_data)\n",
    "dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': tensor([ 0.1000, -0.2000], dtype=torch.float64),\n",
       " 'x2': tensor([7.4000, 5.3000], dtype=torch.float64),\n",
       " 'y': tensor([0, 0])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(dict_data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m nlp_data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_input\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m      3\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m},\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(nlp_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mypytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mypytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mypytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mypytorch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mypytorch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mypytorch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mypytorch/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:138\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "nlp_data = [\n",
    "    {'tokenized_input': [1, 4, 5, 9, 3, 2],\n",
    "     'label':0},\n",
    "    {'tokenized_input': [1, 7, 3, 14, 48, 7, 23, 154, 2],\n",
    "     'label':0},\n",
    "    {'tokenized_input': [1, 30, 67, 117, 21, 15, 2],\n",
    "     'label':1},\n",
    "    {'tokenized_input': [1, 17, 2],\n",
    "     'label':0},\n",
    "]\n",
    "loader = DataLoader(nlp_data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "错误消息表明不可能创建非矩形张量。 顺便说一句，可以看到触发错误的是 default_collate函数。\n",
    "\n",
    "我们可以做什么？ 有两种解决方案：\n",
    "\n",
    "1. 将整个数据集填充到最长的样本。\n",
    "\n",
    "2. 在批创建期间动态填充。\n",
    "\n",
    "第一个解决方案可能看起来更简单—只需将所有样本扩展到最长的样本即可。 但有一个问题—我们会浪费内存和计算能力（它们在 GPU 上很昂贵！）来处理 padding，这并不影响结果。 如果我们的数据中有一些长序列，而且大多数序列都相对较短，那就尤其痛苦。 在这种情况下，我们主要是处理填充而不是数据！\n",
    "\n",
    "如果我们将整个数据集填充到最长的序列，会浪费大量空间！\n",
    "\n",
    "另一种方法是动态填充数据。 当选择该批的样本时，我们只将它们填充到最长的样本。 如果我们另外按长度对数据进行排序，则填充将是最小的。 如果有一些非常长的序列，它们只会影响它们的批次，而不是整个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor([0, 0]),\n",
      " 'tokenized_input': tensor([[  1,   4,   5,   9,   3,   2,   0,   0,   0],\n",
      "        [  1,   7,   3,  14,  48,   7,  23, 154,   2]])}\n",
      "{'label': tensor([1, 0]),\n",
      " 'tokenized_input': tensor([[  1,  30,  67, 117,  21,  15,   2],\n",
      "        [  1,  17,   2,   0,   0,   0,   0]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence #(1)\n",
    "\n",
    "def custom_collate(data): #(2)\n",
    "    inputs = [torch.tensor(d['tokenized_input']) for d in data] #(3)\n",
    "    labels = [d['label'] for d in data]\n",
    "\n",
    "    inputs = pad_sequence(inputs, batch_first=True) #(4)\n",
    "    labels = torch.tensor(labels) #(5)\n",
    "\n",
    "    return { #(6)\n",
    "        'tokenized_input': inputs,\n",
    "        'label': labels\n",
    "    }\n",
    "\n",
    "loader = DataLoader(\n",
    "  \tnlp_data, \n",
    "    batch_size=2, \n",
    "    shuffle=False, \n",
    "    collate_fn=custom_collate\n",
    ") #(7)\n",
    "\n",
    "iter_loader = iter(loader)\n",
    "batch1 = next(iter_loader)\n",
    "pprint(batch1)\n",
    "batch2 = next(iter_loader)\n",
    "pprint(batch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码说明如下：\n",
    "\n",
    "我们使用 pad_sequence进行填充\n",
    "\n",
    "Collate 函数要传入单个参数 - 样本列表。 在这种情况下，它将是一个字典列表，但它也可以是一个元组列表等——具体取决于数据集。\n",
    "\n",
    "当数据出现时，如果格式为“字典列表”，我们需要遍历它并为所有输入和标签创建一个单独的列表。 与此同时， tokenized_input 被转换为一维张量（它是一个整数列表）。\n",
    "\n",
    "执行填充。\n",
    "\n",
    "由于标签是整数列表，我们将其转换为张量。\n",
    "\n",
    "返回格式化的批次。\n",
    "\n",
    "在加载器中设置我们的自定义整理函数。\n",
    "\n",
    "正如我们所看到的，批的格式与字典的默认排序规则相同。 我们清楚地看到填充量很小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_data\n",
    "\n",
    "inputs = [torch.tensor(d['tokenized_input']) for d in nlp_data] #(3)\n",
    "labels = [d['label'] for d in nlp_data]\n",
    "\n",
    "inputs = pad_sequence(inputs, batch_first=True) #(4)\n",
    "labels = torch.tensor(labels) #(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,   4,   5,   9,   3,   2,   0,   0,   0],\n",
       "        [  1,   7,   3,  14,  48,   7,  23, 154,   2],\n",
       "        [  1,  30,  67, 117,  21,  15,   2,   0,   0],\n",
       "        [  1,  17,   2,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "\n",
    "\n",
    "inputs = [torch.tensor(d['tokenized_input']) for d in nlp_data]\n",
    "inputs = pad_sequence(inputs, batch_first=True) #(4)\n",
    "inputs\n",
    "# torch.tensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <class 'str'> <class 'str'>\n",
      "1 <class 'str'> <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "nlp_data = [\n",
    "    {'tokenized_input': [1, 4, 5, 9, 3, 2],\n",
    "     'label':0},\n",
    "    {'tokenized_input': [1, 7, 3, 14, 48, 7, 23, 154, 2],\n",
    "     'label':0},\n",
    "    {'tokenized_input': [1, 30, 67, 117, 21, 15, 2],\n",
    "     'label':1},\n",
    "    {'tokenized_input': [1, 17, 2],\n",
    "     'label':0},\n",
    "]\n",
    "from torch.nn.utils.rnn import pad_sequence #(1)\n",
    "\n",
    "def custom_collate(data): #(2)\n",
    "    inputs = [torch.tensor(d['tokenized_input']) for d in data] #(3)\n",
    "    labels = [d['label'] for d in data]\n",
    "\n",
    "    inputs = pad_sequence(inputs, batch_first=True) #(4)\n",
    "    labels = torch.tensor(labels) #(5)\n",
    "\n",
    "    return { #(6)\n",
    "        'tokenized_input': inputs,\n",
    "        'label': labels\n",
    "    }\n",
    "\n",
    "loader = DataLoader(\n",
    "  \tnlp_data, \n",
    "    batch_size=2, \n",
    "    shuffle=False, \n",
    "    collate_fn=custom_collate\n",
    ") #(7)\n",
    "\n",
    "for i, (X, y) in enumerate(loader):\n",
    "    print(i, type(X), type(y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor([0, 0]),\n",
      " 'tokenized_input': tensor([[  1,   4,   5,   9,   3,   2,   0,   0,   0],\n",
      "        [  1,   7,   3,  14,  48,   7,  23, 154,   2]])}\n",
      "{'label': tensor([1, 0]),\n",
      " 'tokenized_input': tensor([[  1,  30,  67, 117,  21,  15,   2],\n",
      "        [  1,  17,   2,   0,   0,   0,   0]])}\n"
     ]
    }
   ],
   "source": [
    "iter_loader = iter(loader)\n",
    "batch1 = next(iter_loader)\n",
    "pprint(batch1)\n",
    "batch2 = next(iter_loader)\n",
    "pprint(batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
